# üöÄ BIG DATA PIPELINE USING GOOGLE CLOUD DATAPROC

---

## üìå OVERVIEW

This project implements an end-to-end big data processing pipeline using **Google Cloud Platform (GCP)** with **Dataproc (Hadoop + Spark)**.  
The pipeline covers data ingestion, cleaning, transformation, integration, optimization, and data serving in a distributed cloud environment.

---

## üèó ARCHITECTURE

**GCS ‚Üí Dataproc (Hadoop + Spark) ‚Üí Processing ‚Üí Optimized Output ‚Üí External Storage / Visualization**

---

## 1Ô∏è‚É£ DATA INGESTION & EXPLORATION

- Created a Dataproc cluster (Spark + Hadoop)  
- Uploaded CSV files to **Google Cloud Storage (GCS)**  
- Loaded data into Spark DataFrames from GCS/HDFS  
- Extracted and validated schema using `printSchema()` and `describe()`  
- Performed exploratory analysis and LDA (if applicable)

---

## 2Ô∏è‚É£ DATA CLEANING & TRANSFORMATION

- Handled missing/null values  
- Standardized date formats  
- Applied normalization and scaling for numerical features  
- Performed feature engineering to create derived columns  

---

## 3Ô∏è‚É£ DATA INTEGRATION & AGGREGATION

- Joined multiple datasets using Spark SQL  
- Aggregated metrics (count, sum, average, etc.)  
- Removed duplicates and resolved schema inconsistencies  

---

## 4Ô∏è‚É£ PERFORMANCE OPTIMIZATION

- Applied data partitioning for better parallelism  
- Used caching for iterative operations  
- Tuned Spark configurations for efficient job execution  

---

## 5Ô∏è‚É£ DATA SERVING

- Exported processed data to GCS / external databases  
- Created visualizations to analyze trends and patterns  

---

## ‚öôÔ∏è TECH STACK

- Google Cloud Dataproc  
- Apache Spark  
- Hadoop (HDFS)  
- Spark MLlib  
- PySpark  
