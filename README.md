Big Data Pipeline using Google Cloud Dataproc
üìå Overview

This project implements an end-to-end big data processing pipeline using Google Cloud Platform with Dataproc (Hadoop + Spark). The pipeline covers data ingestion, cleaning, transformation, integration, optimization, and data serving in a distributed cloud environment.

üèó Architecture

GCS ‚Üí Dataproc (Hadoop + Spark) ‚Üí Processing ‚Üí Optimized Output ‚Üí External Storage / Visualization

1Ô∏è‚É£ Data Ingestion & Exploration

Created a Dataproc cluster (Spark + Hadoop).

Uploaded CSV files to Google Cloud Storage (GCS).

Loaded data into Spark DataFrames from GCS/HDFS.

Extracted and validated schema using printSchema() and describe().

Performed initial exploratory analysis and LDA (if applicable).

2Ô∏è‚É£ Data Cleaning & Transformation

Handled missing/null values.

Standardized date formats.

Applied normalization and scaling for numerical features.

Performed feature engineering to create new derived columns.

3Ô∏è‚É£ Data Integration & Aggregation

Joined multiple datasets using Spark SQL.

Aggregated metrics (count, sum, average, etc.).

Removed duplicates and resolved schema inconsistencies.

4Ô∏è‚É£ Performance Optimization

Applied data partitioning for better parallelism.

Used caching for iterative operations.

Tuned Spark configurations for efficient job execution.

5Ô∏è‚É£ Data Serving

Exported processed data to GCS / external databases.

Created visualizations to analyze trends and patterns.
